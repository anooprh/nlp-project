{
 "metadata": {
  "name": "",
  "signature": "sha256:aa25b8efaf4e5b65134805da97bcca04ba5d23b1fe2ec516d6b599819d6bfd83"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Connectionist Temporal Classification (CTC) Training in Theano\n",
      "\n",
      "Graves, Alex, et al. [\"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.\"](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_GravesFGS06.pdf) Proceedings of the 23rd international conference on Machine learning. ACM, 2006."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "import theano.tensor as T\n",
      "import numpy         as np\n",
      "from theano_toolkit import utils as U\n",
      "from theano_toolkit import updates"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll create a simple recurrent neural network, and then construct the CTC cost function using the given predictions. The following is pretty standard code for constructing a network with recurrent connections in its hidden layer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_rnn(hidden_inputs,W_hidden_hidden,b_hidden,initial_hidden):\n",
      "    def step(input_curr,hidden_prev):\n",
      "        hidden = T.tanh(\n",
      "                T.dot(hidden_prev,W_hidden_hidden) +\\\n",
      "                input_curr +\\\n",
      "                b_hidden\n",
      "            )\n",
      "        return hidden\n",
      "    hidden,_ = theano.scan(\n",
      "        step,\n",
      "        sequences = [hidden_inputs],\n",
      "        outputs_info = [initial_hidden]\n",
      "    )\n",
      "    return hidden\n",
      "\n",
      "\n",
      "\n",
      "def build_model(input_size,hidden_size,output_size):\n",
      "    X = T.matrix('X')\n",
      "    W_input_hidden  = U.create_shared(U.initial_weights(input_size,hidden_size))\n",
      "    W_hidden_hidden = U.create_shared(U.initial_weights(hidden_size,hidden_size))\n",
      "    W_hidden_output = U.create_shared(U.initial_weights(hidden_size,output_size))\n",
      "    b_hidden = U.create_shared(U.initial_weights(hidden_size))\n",
      "    i_hidden = U.create_shared(U.initial_weights(hidden_size))\n",
      "    b_output = U.create_shared(U.initial_weights(output_size))\n",
      "    \n",
      "    params = [W_input_hidden,W_hidden_hidden,W_hidden_output,b_hidden,i_hidden,b_output]\n",
      "    \n",
      "    hidden = build_rnn(T.dot(X,W_input_hidden),W_hidden_hidden,b_hidden,i_hidden)\n",
      "    \n",
      "    predict = T.nnet.softmax(T.dot(hidden,W_hidden_output) + b_output)\n",
      "    return X,predict,params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### CTC Backward-Forward Algorithm\n",
      "\n",
      "The paper outlines a dynamic programming algorithm used to compute the sum of probabilities over all paths corresponding to a given labelling. This is troublesome to implement in Theano. I've decided to use a matrix here to define the recurrence relations, and this makes for a lot cleaner code.\n",
      "\n",
      "The computation is slightly odd, and differs in alternate time steps. We can recreate this by summing slices of identity matrices of different sizes to get the recurrence relation we want. So given a size 11 input:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size = 11\n",
      "big_I = np.eye(size+2)\n",
      "forward = np.eye(size) + big_I[2:,1:-1] + big_I[2:,:-2] * (np.arange(size) % 2)\n",
      "backward = forward.T\n",
      "forward"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "array([[ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
        "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that this gives us the relation we want. This can be best visualised in Figure 3 of the paper. We'll build this symbolically so that this will be constructed depending on the input size."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def recurrence_relation(size):\n",
      "    big_I = T.eye(size+2)\n",
      "    return T.eye(size) + big_I[2:,1:-1] + big_I[2:,:-2] * (T.arange(size) % 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`path_probs` performs a forward pass, calculating the probabilities of all possible prefix paths using `theano.scan`. You can see the recurrence relation matrix in action here, and it has completely done away with the need for pesky if-else statements. It should be noted that the computation is initialised with a $[1,0,\\cdots,0]$ vector, and the 2 initialising probabilities as described in the paper will be computed according to our recurrence relation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def path_probs(predict,Y):\n",
      "    P = predict[:,Y]\n",
      "    rr = recurrence_relation(Y.shape[0])\n",
      "    def step(p_curr,p_prev):\n",
      "        return p_curr * T.dot(p_prev,rr)\n",
      "    probs,_ = theano.scan(\n",
      "            step,\n",
      "            sequences = [P],\n",
      "            outputs_info = [T.eye(Y.shape[0])[0]]\n",
      "        )\n",
      "    return probs\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So in order to compute both the forward and backward pass, we simpy reverse the matrices to get the computation we want, and then reverse them back. We can then simply perform the computation in equation (14) in the paper to get the probability over all possible paths. We need to maximise this log-probability in order to train the network."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ctc_cost(predict,Y):\n",
      "    forward_probs  = path_probs(predict,Y)\n",
      "    backward_probs = path_probs(predict[::-1],Y[::-1])[::-1,::-1]\n",
      "    probs = forward_probs * backward_probs / predict[:,Y]\n",
      "    total_prob = T.sum(probs)\n",
      "    return -T.log(total_prob)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, just to be sure, let's check if this compute graph plays nicely with `T.grad`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y = T.ivector('Y')\n",
      "X,predict,params = build_model(10,10,10)\n",
      "cost = ctc_cost(predict,Y)\n",
      "\n",
      "# Differentiable\n",
      "grad = T.grad(cost,wrt=params)\n",
      "\n",
      "f = theano.function(\n",
      "        inputs = [X,Y],\n",
      "        outputs = cost\n",
      "    )\n",
      "\n",
      "f(np.eye(11)[:,:10],np.arange(10,dtype=np.int32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "array(23.35882877759107)"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Looks like everything's fine. Train away!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}